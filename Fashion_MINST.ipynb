{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY/n9D8rLt0SRJpSOXM3rQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KasraRezaei/ML/blob/main/Fashion_MINST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fashion-MNIST contains:\n",
        "60,000 training images\n",
        "10,000 test images\n",
        "Each image is 28Ã—28 grayscale\n",
        "10 clothing categories:\n",
        "Label\tItem\n",
        "0\tT-shirt/top\n",
        "1\tTrouser\n",
        "2\tPullover\n",
        "3\tDress\n",
        "4\tCoat\n",
        "5\tSandal\n",
        "6\tShirt\n",
        "7\tSneaker\n",
        "8\tBag\n",
        "9\tAnkle boot\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kBMdt2xwWlSB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I10POOf1UCJa",
        "outputId": "75b37b10-b414-4a04-8d51-29e7a9c097ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7264 - loss: 0.8399 - val_accuracy: 0.8365 - val_loss: 0.4736\n",
            "Epoch 2/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8472 - loss: 0.4510 - val_accuracy: 0.8529 - val_loss: 0.4271\n",
            "Epoch 3/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8588 - loss: 0.4075 - val_accuracy: 0.8527 - val_loss: 0.4206\n",
            "Epoch 4/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8661 - loss: 0.3806 - val_accuracy: 0.8597 - val_loss: 0.3998\n",
            "Epoch 5/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8707 - loss: 0.3606 - val_accuracy: 0.8602 - val_loss: 0.3959\n",
            "Epoch 6/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8733 - loss: 0.3504 - val_accuracy: 0.8730 - val_loss: 0.3655\n",
            "Epoch 7/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8822 - loss: 0.3300 - val_accuracy: 0.8732 - val_loss: 0.3588\n",
            "Epoch 8/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8832 - loss: 0.3223 - val_accuracy: 0.8683 - val_loss: 0.3726\n",
            "Epoch 9/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8879 - loss: 0.3104 - val_accuracy: 0.8696 - val_loss: 0.3696\n",
            "Epoch 10/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8912 - loss: 0.3037 - val_accuracy: 0.8769 - val_loss: 0.3488\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8697 - loss: 0.3662\n",
            "Test score: 0.37085413932800293\n",
            "Test accuracy: 0.8686000108718872\n",
            "Gap (Training - Test): 0.02246248722076416\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import to_categorical\n",
        "# Constants\n",
        "RESHAPED = 784\n",
        "\n",
        "# 10 type cloths\n",
        "NB_CLASSES = 10\n",
        "# Load Fashion-MNIST\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "# Preprocess\n",
        "x_train = x_train.reshape(-1, RESHAPED).astype(\"float32\") / 255\n",
        "x_test = x_test.reshape(-1, RESHAPED).astype(\"float32\") / 255\n",
        "\n",
        "y_train = to_categorical(y_train, NB_CLASSES)\n",
        "# If label = 7 (Sneaker) >> [0,0,0,0,0,0,0,1,0,0]\n",
        "\n",
        "\n",
        "# Shallow model >> one hidden layer\n",
        "# 784 inputs â†’ 64 neurons\n",
        "# Input Parameters: 784 Ã— 64 + 64 = 50,240 (Weights + biases)\n",
        "# Output Parameters: 64 inputs â†’ 10 outputs : 64 Ã— 10(classes) + 10 = 650\n",
        "y_test = to_categorical(y_test, NB_CLASSES)\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(RESHAPED,)))\n",
        "model.add(Dense(NB_CLASSES, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Training\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "#Evalouation:\n",
        "score = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Get training accuracy from the last epoch\n",
        "train_acc = history.history['accuracy'][-1]\n",
        "# Calculate the gap\n",
        "gap = train_acc - score[1]\n",
        "\n",
        "print(\"Test score:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n",
        "print(\"Gap (Training - Test):\", gap)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "â€¢ What test accuracy does this shallow model achieve?"
      ],
      "metadata": {
        "id": "OU7G62wWUebR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: Deepen the Network\n",
        "Now make the network deeper by adding more hidden layers."
      ],
      "metadata": {
        "id": "iwIAaQAZ_dlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Total parameters: Total â‰ˆ 134,794 parameters\n",
        "#  L1: 784 Ã— 128 + 128 = 100,480\n",
        "#  L2: 128 Ã— 128 + 128 = 16,512\n",
        "#  L3: 128 Ã— 128 + 128 = 16,512\n",
        "#  output: 128 Ã— 10 + 10 = 1,290\n",
        "model.add(Dense(128, activation='relu', input_shape=(RESHAPED,)))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(NB_CLASSES, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Training\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "#Evalouation:\n",
        "score = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Get training accuracy from the last epoch\n",
        "train_acc = history.history['accuracy'][-1]\n",
        "# Calculate the gap\n",
        "gap = train_acc - score[1]\n",
        "\n",
        "print(\"Test score:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n",
        "print(\"Gap (Training - Test):\", gap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btxS4pex_e2I",
        "outputId": "50b4ef3c-fbff-4b93-be7f-b968e19934a0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7421 - loss: 0.7716 - val_accuracy: 0.8448 - val_loss: 0.4355\n",
            "Epoch 2/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8543 - loss: 0.4032 - val_accuracy: 0.8670 - val_loss: 0.3749\n",
            "Epoch 3/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8721 - loss: 0.3521 - val_accuracy: 0.8648 - val_loss: 0.3765\n",
            "Epoch 4/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8800 - loss: 0.3238 - val_accuracy: 0.8790 - val_loss: 0.3350\n",
            "Epoch 5/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.8903 - loss: 0.2980 - val_accuracy: 0.8741 - val_loss: 0.3465\n",
            "Epoch 6/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.8938 - loss: 0.2851 - val_accuracy: 0.8776 - val_loss: 0.3297\n",
            "Epoch 7/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.8992 - loss: 0.2713 - val_accuracy: 0.8844 - val_loss: 0.3258\n",
            "Epoch 8/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9026 - loss: 0.2594 - val_accuracy: 0.8871 - val_loss: 0.3201\n",
            "Epoch 9/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9061 - loss: 0.2506 - val_accuracy: 0.8827 - val_loss: 0.3379\n",
            "Epoch 10/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9104 - loss: 0.2376 - val_accuracy: 0.8761 - val_loss: 0.3486\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8705 - loss: 0.3741\n",
            "Test score: 0.37070631980895996\n",
            "Test accuracy: 0.8720999956130981\n",
            "Gap (Training - Test): 0.03656667470932007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "â€¢ How does performance change with added depth?\n",
        "  Adding depth:\n",
        "    - Increased training accuracy: ~88% â†’ ~91%\n",
        "    - Slightly increased validation/test accuracy ~87% â†’ ~88%\n",
        "    Training accuracy: 90.9%\n",
        "    Test accuracy: 87.9%\n",
        "    Gap â‰ˆ 3%\n",
        "    Thatâ€™s mild overfitting beginning.\n",
        "    The deeper model:\n",
        "    Has more capacity\n",
        "    Fits training data better\n",
        "    Generalizes only slightly better\n",
        "\n",
        "\n",
        "â€¢ Does training time increase?\n",
        "  Yes â€” clearly, ~2â€“3 seconds per epoch\n",
        "  Why?\n",
        "    - First layer: 784 Ã— 64 + 64 = 50,240\n",
        "  New deep model (3 Ã— 128 layers)\n",
        "    Layer 1:\n",
        "    784 Ã— 128 + 128 = 100,480\n",
        "    Layer 2:\n",
        "    128 Ã— 128 + 128 = 16,512\n",
        "    Layer 3:\n",
        "    128 Ã— 128 + 128 = 16,512\n",
        "    Output:\n",
        "    128 Ã— 10 + 10 = 1,290\n",
        "    Total â‰ˆ 134,794 parameters\n",
        "\n",
        "Key Insight\n",
        "Increasing depth:\n",
        "âœ” Improves training accuracy\n",
        "âœ” Slightly improves test accuracy\n",
        "âœ” Increases training time\n",
        "âœ” Increases overfitting risk\n",
        "But improvement is diminishing.\n",
        "\n",
        "Big Concept\n",
        "Fully connected networks struggle with images because:\n",
        "They treat images as flat vectors\n",
        "They donâ€™t preserve spatial locality\n",
        "They canâ€™t detect patterns like edges or textures explicitly\n",
        "Thatâ€™s why CNNs outperform them.\n",
        "\n",
        "Why We Need Validation\n",
        "Imagine this:\n",
        "You train for many epochs.\n",
        "If:\n",
        "Training accuracy â†‘\n",
        "Validation accuracy â†“\n",
        "That means:\n",
        "The model is memorizing training data â†’ overfitting.\n",
        "Validation helps you decide:\n",
        "When to stop training\n",
        "Whether to change architecture\n",
        "Whether to regularize\n"
      ],
      "metadata": {
        "id": "JSY5u8ktAEzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3: Widen the Network\n",
        "Now experiment with increasing the width (number of neurons per layer):"
      ],
      "metadata": {
        "id": "NzXxwabWAZvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(RESHAPED,)))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(NB_CLASSES, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#Training\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "#Evalouation:\n",
        "score = model.evaluate(x_test, y_test)\n",
        "\n",
        "# Get training accuracy from the last epoch\n",
        "train_acc = history.history['accuracy'][-1]\n",
        "# Calculate the gap\n",
        "gap = train_acc - score[1]\n",
        "\n",
        "print(\"Test score:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])\n",
        "print(\"Gap (Training - Test):\", gap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXn_UoVKAdJO",
        "outputId": "f01c40d6-ef1b-43b1-ab3f-099c8d69022c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 18ms/step - accuracy: 0.7636 - loss: 0.6710 - val_accuracy: 0.8628 - val_loss: 0.3775\n",
            "Epoch 2/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 21ms/step - accuracy: 0.8647 - loss: 0.3716 - val_accuracy: 0.8733 - val_loss: 0.3543\n",
            "Epoch 3/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.8800 - loss: 0.3218 - val_accuracy: 0.8714 - val_loss: 0.3498\n",
            "Epoch 4/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 17ms/step - accuracy: 0.8893 - loss: 0.2944 - val_accuracy: 0.8829 - val_loss: 0.3257\n",
            "Epoch 5/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.8972 - loss: 0.2752 - val_accuracy: 0.8864 - val_loss: 0.3208\n",
            "Epoch 6/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9052 - loss: 0.2574 - val_accuracy: 0.8825 - val_loss: 0.3289\n",
            "Epoch 7/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 18ms/step - accuracy: 0.9066 - loss: 0.2481 - val_accuracy: 0.8849 - val_loss: 0.3132\n",
            "Epoch 8/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 20ms/step - accuracy: 0.9127 - loss: 0.2339 - val_accuracy: 0.8877 - val_loss: 0.3163\n",
            "Epoch 9/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.9160 - loss: 0.2225 - val_accuracy: 0.8939 - val_loss: 0.3039\n",
            "Epoch 10/10\n",
            "\u001b[1m375/375\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.9204 - loss: 0.2110 - val_accuracy: 0.8893 - val_loss: 0.3200\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8825 - loss: 0.3502\n",
            "Test score: 0.34889182448387146\n",
            "Test accuracy: 0.8844000101089478\n",
            "Gap (Training - Test): 0.033870816230773926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Did Performance Improve?\n",
        "Test accuracy before (128Ã—3 model):\n",
        "â‰ˆ 87.9%\n",
        "Test accuracy now (512Ã—2 model):\n",
        "â‰ˆ 88.2%\n",
        "ğŸ” Improvement:\n",
        "Only about +0.3%\n",
        "Very small.\n",
        "\n",
        "\n",
        "Training Accuracy vs Test Accuracy\n",
        "Final epoch:\n",
        "Training accuracy â‰ˆ 91.7%\n",
        "Validation accuracy â‰ˆ 88.8%\n",
        "Test accuracy â‰ˆ 88.2%\n",
        "Gap:\n",
        "Train â‰ˆ 91.7%\n",
        "Test  â‰ˆ 88.2%\n",
        "Gap   â‰ˆ 3.5%\n",
        "That gap is bigger than before.\n",
        "ğŸ‘‰ This means overfitting is increasing.\n",
        "The model has more capacity and fits training data better, but generalization barely improves.\n",
        "\n",
        "Did Training Time Increase?\n",
        "Yes â€” significantly.\n",
        "Previous model (128Ã—3):\n",
        "~3â€“5 seconds per epoch\n",
        "New model (512Ã—2):\n",
        "~7â€“11 seconds per epoch\n",
        "So training time roughly doubled or tripled.\n",
        "Total:\n",
        "â‰ˆ 669,706 parameters\n",
        "Thatâ€™s about 5Ã— larger than your 128Ã—3 model.\n",
        "\n",
        "Important Insight:\n",
        "You are now seeing a classic ML pattern:\n",
        "Increasing model size always improves training accuracy\n",
        "But test accuracy improves only up to a point\n",
        "After that:\n",
        "You just memorize better\n",
        "You donâ€™t generalize better\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CGC8UN70OGek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions\n",
        "â€¢ How does widening compare to deepening?\n",
        "Deepening (adding more hidden layers) slightly improves performance by allowing the network to learn more complex feature hierarchies.\n",
        "Widening (adding more neurons per layer) increases the networkâ€™s capacity to store information but mostly helps with fitting training data rather than generalization.\n",
        "â€¢ Does accuracy improve significantly?\n",
        "Accuracy improves modestly with depth and width in fully connected networks on Fashion-MNIST.\n",
        "Beyond a certain point (e.g., 512 neurons per layer), the improvement is very small (~0.3â€“1%) because the problem is relatively simple and fully connected layers donâ€™t exploit spatial patterns.\n",
        "â€¢ What about computational cost (training time, memory usage)?\n",
        "Both widening and deepening increase computational cost.\n",
        "Wider layers dramatically increase memory and multiply computations because the number of parameters grows quickly.\n",
        "Deeper networks increase training time more gradually but add complexity.\n",
        "\n",
        "Summary Reflections:\n",
        "Increasing depth lets the model learn more complex representations, slightly improving generalization, but each added layer contributes diminishing returns. Increasing width allows the model to fit training data better, but generalization gains are limited, and computational cost rises sharply. The model stops improving when capacity exceeds what the data requires; at that point, accuracy saturates and overfitting may start, especially in fully connected networks. For Fashion-MNIST, fully connected layers reach ~88â€“89% test accuracy, and further depth or width brings little benefit.\n"
      ],
      "metadata": {
        "id": "2NduTdn1O7-c"
      }
    }
  ]
}